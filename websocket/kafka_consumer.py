import asyncio
import json
import logging
import os
import uuid
import time
from collections import OrderedDict
from aiokafka import AIOKafkaConsumer
from websocket_manager import manager

logger = logging.getLogger(__name__)

# Dedupe ìºì‹œ (LRU)
recent_events: OrderedDict[str, float] = OrderedDict()
MAX_CACHE_SIZE = 10000
CACHE_TTL = 300  # 5ë¶„ (ì´ˆ)

async def consume_realtime_events():
   
     # ============ 3ë‹¨ê³„: Kafka Consumer ì„¤ì • ============
    bootstrap_servers = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "localhost:9092")
    unique_group_id = f"websocket-server-{uuid.uuid4().hex[:8]}"
    
    consumer = AIOKafkaConsumer(
        "realtime.events",       # 3. êµ¬ë…í•  í† í”½
        bootstrap_servers=bootstrap_servers,
        group_id=unique_group_id,  # âœ… HPA: ê° ì¸ìŠ¤í„´ìŠ¤ ë‹¤ë¥¸ ID
        value_deserializer=lambda v: json.loads(v.decode('utf-8')),
        auto_offset_reset='latest'
    )
    # ì—¬ê¸°ê¹Œì§€ëŠ” ê·¸ëƒ¥ ì„¤ì •ë§Œ! ì•„ì§ ì—°ê²° ì•ˆ ë¨
    
     # ============ 4ë‹¨ê³„: Kafka ì—°ê²° ì‹œì‘ ============
    await consumer.start() # â† Kafka ì„œë²„ì— ì—°ê²°!
    logger.info(f"âœ… Kafka Consumer ì‹œì‘: group_id={unique_group_id}")
    
    try:         
        async for msg in consumer: 
            # ë¬´í•œ ë£¨í”„: Kafkaì— ë©”ì‹œì§€ ì˜¬ ë•Œê¹Œì§€ ê³„ì† ê¸°ë‹¤ë¦¼...
            # ë©”ì‹œì§€ ì—†ìœ¼ë©´ ì—¬ê¸°ì„œ ê³„ì† ëŒ€ê¸°!
            try: 
                # ============ 5ë‹¨ê³„: ë©”ì‹œì§€ ë°›ìŒ! ============
                event_data = msg.value # â† ë©”ì‹œì§€ ë„ì°©í•˜ë©´ ì—¬ê¸° ì‹¤í–‰!
            
                event_id = event_data.get("event_id")
                # Dedupe: ì¤‘ë³µ ì´ë²¤íŠ¸ í•„í„°ë§
                if event_id and event_id in recent_events:
                    logger.debug(f"â­ï¸ ì¤‘ë³µ ì´ë²¤íŠ¸ ë¬´ì‹œ: {event_id}")
                    continue
                
                # ìºì‹œ ì¶”ê°€ (LRU)
                if event_id:
                    recent_events[event_id] = time.time()
                    
                    # ìºì‹œ í¬ê¸° ì œí•œ
                    if len(recent_events) > MAX_CACHE_SIZE:
                        recent_events.popitem(last=False)  # ê°€ì¥ ì˜¤ë˜ëœ ê²ƒ ì œê±°
                
                # ì±„ë„ ì¶”ì¶œ
                channel = event_data.get("channel", "stock")
                
                # ============ 6ë‹¨ê³„: Frontendë¡œ ì „ì†¡í•˜ëŠ”ë° websocket_manager.pyì˜ broadcast í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ”ê²ƒ
                # ê·¸ë˜ì„œ ì´ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ë©´ 7,8ë‹¨ê³„ ì‹¤í–‰í•œë‹¤. ê·¸ë‹¤ìŒ ëë‚˜ë©´ ë‹¤ì‹œ 5ë‹¨ê³„ë¡œ ëŒì•„ê°
                await manager.broadcast(channel, event_data)
                
                logger.debug(f"ğŸ“¤ ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì™„ë£Œ: channel={channel}, event_id={event_id}")
                # ============ ë‹¤ì‹œ 5ë‹¨ê³„ë¡œ ëŒì•„ê° ============
                # async for ë£¨í”„ê°€ ë‹¤ì‹œ ëŒë©´ì„œ ë‹¤ìŒ ë©”ì‹œì§€ ëŒ€ê¸°
            except Exception as e:
                logger.error(f"âŒ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
    
    except Exception as e:
        logger.error(f"âŒ Kafka Consumer ì—ëŸ¬: {e}", exc_info=True)
    
    finally:
        await consumer.stop()
        logger.info("Kafka Consumer ì¢…ë£Œ")
