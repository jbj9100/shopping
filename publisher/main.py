import asyncio
import os
import logging
import uuid
from dotenv import load_dotenv
from db.conn_db import get_db, ping_db, dispose_engine
from db.conn_kafka import (
    KAFKA_BOOTSTRAP_SERVERS, 
    KAFKA_USER, 
    KAFKA_PASSWORD, 
    KAFKA_SASL_MECHANISM,
    list_kafka_topics  # í† í”½ ëª©ë¡ ì¡°íšŒ
)
from repositories.rep_outbox import OutboxRepository
from aiokafka import AIOKafkaProducer
from aiokafka.errors import KafkaError
import json

load_dotenv()

LOG_LEVEL = os.getenv("LOG_LEVEL").upper()
LOG_FORMAT = os.getenv("LOG_FORMAT")
logging.basicConfig(
    level=getattr(logging, LOG_LEVEL),
    format=LOG_FORMAT
)
logger = logging.getLogger(__name__)

# Publisher ì„¤ì •
POLL_INTERVAL_SECONDS = int(os.getenv("POLL_INTERVAL_SECONDS")) 
BATCH_SIZE = int(os.getenv("BATCH_SIZE")) 
PUBLISHER_ID = f"publisher-{uuid.uuid4().hex[:8]}"  


class OutboxPublisher:
    """Transactional Outbox Pattern - Publisher"""

    def __init__(self):
        self.producer: AIOKafkaProducer | None = None
        self.repository = OutboxRepository()

    async def init_kafka_producer(self) -> None:
        """Kafka Producer ì´ˆê¸°í™”"""
        bootstrap_servers = [s.strip() for s in KAFKA_BOOTSTRAP_SERVERS.split(",")]
        
        logger.info(f"Kafka ë¸Œë¡œì»¤ ì—°ê²° ì‹œë„: {bootstrap_servers}")
        logger.info(f"SASL ì¸ì¦ ì‚¬ìš©: mechanism={KAFKA_SASL_MECHANISM}, user={KAFKA_USER}")
        self.producer = AIOKafkaProducer(
            bootstrap_servers=bootstrap_servers,
            security_protocol="SASL_PLAINTEXT",
            sasl_mechanism=KAFKA_SASL_MECHANISM,
            sasl_plain_username=KAFKA_USER,
            sasl_plain_password=KAFKA_PASSWORD,
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            request_timeout_ms=10000,
        )

        await self.producer.start()
        logger.info("âœ… Kafka Producer ì‹œì‘ ì™„ë£Œ")

    async def close_kafka_producer(self) -> None:
        """Kafka Producer ì¢…ë£Œ"""
        if self.producer:
            await self.producer.stop()
            logger.info("Kafka Producer ì¢…ë£Œ")

    async def process_event(self, db, event) -> None:
        """
        ë‹¨ì¼ ì´ë²¤íŠ¸ ì²˜ë¦¬
        
        1. PROCESSING ìƒíƒœë¡œ ë³€ê²½ (ë½ íšë“)
        2. Kafkaë¡œ ì „ì†¡
        3. ì„±ê³µ ì‹œ PUBLISHEDë¡œ ë³€ê²½
        4. ì‹¤íŒ¨ ì‹œ FAILEDë¡œ ë³€ê²½ + ì¬ì‹œë„ ìŠ¤ì¼€ì¤„ë§
        """
        # 1. ë½ íšë“ ì‹œë„
        locked = await self.repository.mark_as_processing(db, event.id, PUBLISHER_ID)
        if not locked:
            return  # ë‹¤ë¥¸ Publisherê°€ ì²˜ë¦¬ì¤‘
        
        try:
            # 2. Kafkaë¡œ ì „ì†¡
            topic = event.topic or f"{event.aggregate_type.lower()}-events"
            
            await self.producer.send_and_wait(
                topic=topic,
                value=event.payload,
                key=str(event.aggregate_id).encode('utf-8')
            )
            
            # 3. ì„±ê³µ ì²˜ë¦¬
            await self.repository.mark_as_published(db, event.id)
            logger.info(f"ğŸ“¤ ì´ë²¤íŠ¸ ë°œí–‰ ì„±ê³µ: {event.event_type} (topic={topic})")
            
        except KafkaError as e:
            # 4. ì‹¤íŒ¨ ì²˜ë¦¬
            error_msg = f"Kafka ì „ì†¡ ì‹¤íŒ¨: {str(e)}"
            await self.repository.mark_as_failed(db, event.id, error_msg, retry_delay_seconds=60)
            
        except Exception as e:
            # ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬
            error_msg = f"ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {type(e).__name__}: {str(e)}"
            await self.repository.mark_as_failed(db, event.id, error_msg, retry_delay_seconds=300)

    async def poll_and_publish(self) -> None:
        """DB polling í›„ ì´ë²¤íŠ¸ ë°œí–‰ (ë©”ì¸ ë£¨í”„)"""
        logger.info(f"ğŸš€ Publisher ì‹œì‘ (ID={PUBLISHER_ID}, interval={POLL_INTERVAL_SECONDS}s)")
        
        while True: # ë¬´í•œ ë£¨í”„
            try:
                # 1. DBì—ì„œ pending ì´ë²¤íŠ¸ ì¡°íšŒ
                async for db in get_db():
                    events = await self.repository.get_pending_events(db, limit=BATCH_SIZE)
                    
                    if not events:
                        logger.debug("ë°œí–‰í•  ì´ë²¤íŠ¸ ì—†ìŒ")
                        break         # ì´ë²¤íŠ¸ ì—†ìœ¼ë©´ ì¡°íšŒ ì¢…ë£Œ
                    
                    # 2. ê° ì´ë²¤íŠ¸ kafka ë°œí–‰ ì²˜ë¦¬ (ìˆœì°¨)
                    for event in events:
                        await self.process_event(db, event)
                
                # 3. ì„¤ì •í•œ interval 1ì´ˆ í›„ ë‹¤ì‹œ ë°˜ë³µ
                await asyncio.sleep(POLL_INTERVAL_SECONDS)
                
            except Exception as e:
                logger.error(f"âŒ Polling ì¤‘ ì—ëŸ¬: {type(e).__name__}: {str(e)}")
                await asyncio.sleep(5)  # ì—ëŸ¬ ì‹œ 5ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„

    async def run(self) -> None:
        """Publisher ì‹¤í–‰"""
        try:
            # 1. DB ì—°ê²° í…ŒìŠ¤íŠ¸
            logger.info("PostgreSQL ì—°ê²° í…ŒìŠ¤íŠ¸...")
            success, error = await ping_db()
            if not success:
                raise Exception(f"DB ì—°ê²° ì‹¤íŒ¨: {error}")
            
            # 2. Kafka Producer ì´ˆê¸°í™”
            await self.init_kafka_producer()
            
            # 3. Kafka í† í”½ ëª©ë¡ í™•ì¸ (ê°„ë‹¨íˆ)
            success, topics, error = await list_kafka_topics()
            if not success:
                logger.warning(f"âš ï¸  í† í”½ ì¡°íšŒ ì‹¤íŒ¨: {error}")
            
            # 4. ë©”ì¸ ë£¨í”„ ì‹¤í–‰
            await self.poll_and_publish()
            
        except KeyboardInterrupt:
            logger.info("Publisher ì¢…ë£Œ ì¤‘...")
        except Exception as e:
            logger.error(f"âŒ Publisher ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {type(e).__name__}: {str(e)}")
            raise
        finally:
            # ì •ë¦¬
            await self.close_kafka_producer()
            await dispose_engine()
            logger.info("Publisher ì™„ì „ ì¢…ë£Œ")


async def main():
    """ë©”ì¸ ì—”íŠ¸së¦¬í¬ì¸íŠ¸"""
    publisher = OutboxPublisher()
    await publisher.run()


if __name__ == "__main__":
    asyncio.run(main())
